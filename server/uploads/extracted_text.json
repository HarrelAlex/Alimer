[
    {
        "page": 1,
        "text": "SYNOPSIS\nTitle:Empowering Machine Learning: Decentralized Model Training\nthrough Federated Learning\nFederated Learning (FL) is an emerging distributed machine learning paradigm that enables multiple\nclients to collaboratively train a shared model while keeping their data decentralized and private. In\ncontrast to traditional centralized machine learning approaches, FL allows clients to train models locally\non their own data and only share model updates with a central server. This server then aggregates the\nupdates to improve the global model, without ever accessing the raw data from individual clients.\nThe key idea behind FL is to bring the learning process to the data, rather than the data to the learning\nprocess. This approach is particularly advantageous in scenarios where data is sensitive, private, or\ngeographically distributed, such as in healthcare, finance, and mobile applications. By keeping the data\non the client devices, FL mitigates the risks associated with data breaches and enables the use of\nmachine learning in privacy-sensitive domains.\nHowever, the implementation of federated learning faces several challenges, including high\ncommunication costs, statistical heterogeneity of data across clients, and the need for robust privacy-\npreserving mechanisms. Researchers are actively working on addressing these challenges through the\ndevelopment of novel algorithms, system architectures, and privacy-enhancing techniques.\nSome of the key areas of research in federated learning include:\nFederated Optimization Algorithms: Designing efficient algorithms for aggregating model updates and\nconverging to an optimal global model, while considering the constraints of the federated setting.\nHeterogeneity and Non-IID Data: Developing techniques to handle the statistical heterogeneity of data\nacross clients, which can lead to biased models and slow convergence.\nPrivacy and Security: Ensuring the privacy of client data and the security of the federated learning\nsystem against potential attacks, such as model inversion and membership inference.\nScalability and System Design: Addressing the challenges of scaling federated learning to large\nnumbers of clients and devices, and designing efficient system architectures for deployment.\nApplications and Use Cases: Exploring the potential of federated learning in various domains, such as\nhealthcare, finance, and mobile applications, and demonstrating its practical benefits.\nAs the field of federated learning continues to evolve, it holds great promise for enabling secure and\ncollaborative machine learning at scale, while respecting the privacy of individual users and\norganizations. The ongoing research in this area is expected to have a significant impact on the future\nof machine learning and data privacy.\nPresented by STEVE PAUL\nBatch R7B\nRoll No 361(SCT21AM061)\nReferences"
    },
    {
        "page": 2,
        "text": "[1] A. Mohri, M.N.S.R Stokes, and Others. 2019. \u201cPapers with Code - Agnostic Federated\nLearning.\u201d Paperswithcode.com. 2019. https://paperswithcode.com/paper/agnostic-federated-\nlearning.\n[2] H. Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise Aguera y\nArcas. 2016. \u201cPapers with Code - Communication-Efficient Learning of Deep Networks from\nDecentralized Data.\u201d Paperswithcode.com. 2016.\nhttps://paperswithcode.com/paper/communication-efficient-learning-of-deep.\n[3] Jakub Kone\u010dn\u00fd, Brendan McMahan, Daniel Ramage, Peter Richt\u00e1rik, and et al. 2019. \u201cPapers\nwith Code - towards Federated Learning at Scale: System Design.\u201d Paperswithcode.com. 2019.\nhttps://paperswithcode.com/paper/towards-federated-learning-at-scale-system.\n[4] Peter Kairouz, Brendan McMahan, and et al. 2021. \u201cPapers with Code - Advances and Open\nProblems in Federated Learning.\u201d Paperswithcode.com. 2021.\nhttps://paperswithcode.com/paper/advances-and-open-problems-in-federated.\n[5] Yujia Zhang, Yujing Wang, Shuang Li, and et al. 2020. \u201cPapers with Code - FedMD:\nHeterogenous Federated Learning via Model Distillation.\u201d Paperswithcode.com. 2020.\nhttps://paperswithcode.com/paper/fedmd-heterogenous-federated-learning-via."
    }
]