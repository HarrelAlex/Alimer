[
    {
        "page": 1,
        "text": "DEEP Q NETWORK FOR AUTONOMOUS VEHICLES\nThe world of transportation is on the verge of a profound transformation, and at the\nheart of this revolution are autonomous vehicles. Over the years, significant\nadvancements have been made in autonomous vehicle technology, bringing us closer to\na future where vehicles can navigate our roads with minimal human intervention. These\nvehicles have the capability to navigate and operate without human intervention, relying\non a combination of advanced sensors, machine learning algorithms, and onboard\ncomputer systems. Technological improvements in AI algorithms have led to\nrevolutionize transportation by reducing accidents, enhancing traffic flow, and\nproviding newfound mobility to individuals who were previously unable to drive.\nThe primary aim of this seminar is to serve as a stepping stone into the field of Deep Q\nnetworks. DQN was introduced by Volodymyr Mnih in 2015 and has since become a\nfoundational algorithm in the field of deep reinforcement learning. It builds upon the\nprinciples of reinforcement learning and Q-learning, with the addition of deep neural\nnetworks to approximate the Q-values, which represent the expected cumulative\nrewards for taking actions in a given state where the vehicle needs to navigate its\nenvironment and make choices.This seminar provides a comprehensive overview of\ncore concept of Deep Q networks, discussing the significance and technical\nimprovements of DQN in the context of autonomous driving.\nPresented by Hridya Sreekumar\nBatch R7A\nRoll No SCT20CS039\nGuided by Mrs. Soja Salim\n(Assistant Professor, Dept. of CSE,\nSCTCE)\nReferences\n[1] Juan Wu,Seabyuk Shin, Cheong-Gil Kim and Shin-Dug Kim, \"Effective Lazy\nTraining Method for Deep Q-Network in Obstacle Avoidance and Path\nPlanning\", IEEE International Conference on Systems, Man, and Cybernetics\n(SMC) Banff Center, Banff, Canada,2017.\n[2] Takafumi Okuyama, Tad Gonsalves and Jaychand Upadhay , \"Autonomous\nDriving System based on Deep Q Learning \" , International Conference on\nIntelligent Autonomous Systems, 2018.\n[3] Shuojie Mo, Xiaofei Pei and Zhenfu Chen. \"Decision-Making for Oncoming\nTraffic Overtaking Scenario using Double DQN \", 3rd Conference on Vehicle\nControl and Intelligence (CVCI),2019."
    },
    {
        "page": 2,
        "text": "[4] Badr Ben Elallid, Nabil Benamar, Nabil Mrani and Tajjeeddine Rachidi, \"DQN-\nbased Reinforcement Learning for Vehicle Control of Autonomous Vehicles\nInteracting With Pedestrians\", International Conference on Innovation and\nIntelligence for Informatics, Computing, and Technologies (3ICT),2022.\n[5] Wasinee Terapaptommakol,Danai Phaoharuhansa,Pramote Koowattanasuchat\nand Jartuwat Rajruangrabin. \"Design of Obstacle Avoidance for Autonomous\nVehicle Using Deep Q-Network and CARLA Simulator \", World Electric Vehicle\nJournal, 2022.\n[6] Henan Yuan, Penghui Li, Bart van Arem,Liujiang Kang, and Yongqi Dong.\n\"Safe, Efficient, Comfort, and Energy-saving Automated Driving through\nRoundabout Based on Deep Reinforcement Learning \" , IEEE 26th International\nConference on Intelligent Transportation Systems (ITSC), 2023."
    }
]