{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Check for GPU availability\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "\n",
    "# Allow memory growth for the GPU\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    except RuntimeError as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 1/5000, Avg Reward: 14.8724, Epsilon: 0.9950\n",
      "Episode: 2/5000, Avg Reward: 15.0420, Epsilon: 0.9900\n",
      "Episode: 3/5000, Avg Reward: 14.9794, Epsilon: 0.9851\n",
      "Episode: 4/5000, Avg Reward: 15.1975, Epsilon: 0.9801\n",
      "Episode: 5/5000, Avg Reward: 15.0317, Epsilon: 0.9752\n",
      "Episode: 6/5000, Avg Reward: 15.0288, Epsilon: 0.9704\n",
      "Episode: 7/5000, Avg Reward: 15.0453, Epsilon: 0.9655\n",
      "Episode: 8/5000, Avg Reward: 15.0884, Epsilon: 0.9607\n",
      "Episode: 9/5000, Avg Reward: 15.1168, Epsilon: 0.9559\n",
      "Episode: 10/5000, Avg Reward: 15.1174, Epsilon: 0.9511\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 213\u001b[0m\n\u001b[0;32m    210\u001b[0m \u001b[38;5;66;03m# Prepare batch data\u001b[39;00m\n\u001b[0;32m    211\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, (state_i, actions_i, rewards_i, next_state_i, done_i) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(minibatch):\n\u001b[0;32m    212\u001b[0m     \u001b[38;5;66;03m# Get current Q-values\u001b[39;00m\n\u001b[1;32m--> 213\u001b[0m     current_q_ad_type, current_q_ad_topic, current_q_ad_placement \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate_i\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    215\u001b[0m     \u001b[38;5;66;03m# Get future Q-values for next state\u001b[39;00m\n\u001b[0;32m    216\u001b[0m     future_q_ad_type, future_q_ad_topic, future_q_ad_placement \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(next_state_i, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[1;32me:\\Program Files\\Python\\Python312\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:117\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    115\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32me:\\Program Files\\Python\\Python312\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:559\u001b[0m, in \u001b[0;36mTensorFlowTrainer.predict\u001b[1;34m(self, x, batch_size, verbose, steps, callbacks)\u001b[0m\n\u001b[0;32m    557\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    558\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m epoch_iterator\u001b[38;5;241m.\u001b[39mcatch_stop_iteration():\n\u001b[1;32m--> 559\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miterator\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mepoch_iterator\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m    560\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mon_predict_batch_begin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    561\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mget_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32me:\\Program Files\\Python\\Python312\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:736\u001b[0m, in \u001b[0;36mTFEpochIterator.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    735\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__next__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m--> 736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_epoch_iterator\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32me:\\Program Files\\Python\\Python312\\Lib\\site-packages\\keras\\src\\trainers\\epoch_iterator.py:112\u001b[0m, in \u001b[0;36mEpochIterator._enumerate_iterator\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    110\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m step, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_current_iterator\n\u001b[0;32m    111\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_batches \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_steps_seen \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_batches:\n\u001b[1;32m--> 112\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_current_iterator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43miter\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    113\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_steps_seen \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32me:\\Program Files\\Python\\Python312\\Lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py:501\u001b[0m, in \u001b[0;36mDatasetV2.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    499\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m context\u001b[38;5;241m.\u001b[39mexecuting_eagerly() \u001b[38;5;129;01mor\u001b[39;00m ops\u001b[38;5;241m.\u001b[39minside_function():\n\u001b[0;32m    500\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m ops\u001b[38;5;241m.\u001b[39mcolocate_with(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variant_tensor):\n\u001b[1;32m--> 501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43miterator_ops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mOwnedIterator\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    502\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    503\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`tf.data.Dataset` only supports Python-style \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    504\u001b[0m                      \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miteration in eager mode or within tf.function.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32me:\\Program Files\\Python\\Python312\\Lib\\site-packages\\tensorflow\\python\\data\\ops\\iterator_ops.py:709\u001b[0m, in \u001b[0;36mOwnedIterator.__init__\u001b[1;34m(self, dataset, components, element_spec)\u001b[0m\n\u001b[0;32m    705\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m (components \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m element_spec \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    706\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    707\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhen `dataset` is provided, `element_spec` and `components` must \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    708\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnot be specified.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 709\u001b[0m   \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    711\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_next_call_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[1;32me:\\Program Files\\Python\\Python312\\Lib\\site-packages\\tensorflow\\python\\data\\ops\\iterator_ops.py:748\u001b[0m, in \u001b[0;36mOwnedIterator._create_iterator\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    745\u001b[0m   \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(fulltype\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39margs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(\n\u001b[0;32m    746\u001b[0m       \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flat_output_types)\n\u001b[0;32m    747\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterator_resource\u001b[38;5;241m.\u001b[39mop\u001b[38;5;241m.\u001b[39mexperimental_set_type(fulltype)\n\u001b[1;32m--> 748\u001b[0m \u001b[43mgen_dataset_ops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmake_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mds_variant\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_iterator_resource\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32me:\\Program Files\\Python\\Python312\\Lib\\site-packages\\tensorflow\\python\\ops\\gen_dataset_ops.py:3509\u001b[0m, in \u001b[0;36mmake_iterator\u001b[1;34m(dataset, iterator, name)\u001b[0m\n\u001b[0;32m   3507\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tld\u001b[38;5;241m.\u001b[39mis_eager:\n\u001b[0;32m   3508\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3509\u001b[0m     _result \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_FastPathExecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   3510\u001b[0m \u001b[43m      \u001b[49m\u001b[43m_ctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mMakeIterator\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _result\n\u001b[0;32m   3512\u001b[0m   \u001b[38;5;28;01mexcept\u001b[39;00m _core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "from collections import deque\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import os\n",
    "\n",
    "# Load the preprocessed dataset\n",
    "# Assuming the CSV is in the same directory as the script\n",
    "preprocessed_data = pd.read_csv(r\".\\preprocessed_marketing_dataset.csv\")\n",
    "\n",
    "# Define state columns and action columns\n",
    "state_columns = ['Age', 'Gender', 'Income', 'Location', 'Clicks']\n",
    "action_columns = ['Ad Type', 'Ad Topic', 'Ad Placement']\n",
    "\n",
    "# Create mappings for categorical variables\n",
    "label_encoders = {}\n",
    "inverse_encoders = {}\n",
    "\n",
    "for column in preprocessed_data.columns:\n",
    "    if preprocessed_data[column].dtype == 'object':\n",
    "        unique_values = preprocessed_data[column].unique()\n",
    "        label_encoders[column] = {value: idx for idx, value in enumerate(unique_values)}\n",
    "        inverse_encoders[column] = {idx: value for idx, value in enumerate(unique_values)}\n",
    "\n",
    "# Apply encodings to the dataset\n",
    "encoded_data = preprocessed_data.copy()\n",
    "for column, encoder in label_encoders.items():\n",
    "    encoded_data[column] = encoded_data[column].map(encoder)\n",
    "\n",
    "# Calculate feature ranges for normalization\n",
    "feature_ranges = {}\n",
    "for column in encoded_data.columns:\n",
    "    if encoded_data[column].dtype != 'object':\n",
    "        feature_ranges[column] = (encoded_data[column].min(), encoded_data[column].max())\n",
    "\n",
    "# Normalize numerical features\n",
    "for column, (min_val, max_val) in feature_ranges.items():\n",
    "    if max_val > min_val:  # Avoid division by zero\n",
    "        encoded_data[column] = (encoded_data[column] - min_val) / (max_val - min_val)\n",
    "\n",
    "# Define improved MDP environment\n",
    "class ImprovedMarketingMDP:\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "        self.current_idx = None\n",
    "        self.num_users = len(data)\n",
    "        \n",
    "        # Get action space sizes\n",
    "        self.num_ad_types = len(data['Ad Type'].unique())\n",
    "        self.num_ad_topics = len(data['Ad Topic'].unique())\n",
    "        self.num_ad_placements = len(data['Ad Placement'].unique())\n",
    "    \n",
    "    def reset(self):\n",
    "        # Start with a random user profile\n",
    "        self.current_idx = np.random.randint(0, self.num_users)\n",
    "        current_state = self.data.iloc[self.current_idx][state_columns].values\n",
    "        return current_state\n",
    "    \n",
    "    def step(self, ad_type_idx, ad_topic_idx, ad_placement_idx):\n",
    "        # Get current user profile\n",
    "        user_profile = self.data.iloc[self.current_idx]\n",
    "        \n",
    "        # Try to find similar examples in the dataset\n",
    "        similar_examples = self.data[\n",
    "            (self.data['Gender'] == user_profile['Gender']) & \n",
    "            (self.data['Location'] == user_profile['Location']) & \n",
    "            (self.data['Ad Type'] == ad_type_idx) & \n",
    "            (self.data['Ad Topic'] == ad_topic_idx) & \n",
    "            (self.data['Ad Placement'] == ad_placement_idx)\n",
    "        ]\n",
    "        \n",
    "        # If we have similar examples, use their average conversion rate as reward\n",
    "        if len(similar_examples) > 0:\n",
    "            reward = similar_examples['Conversion Rate'].mean()\n",
    "        else:\n",
    "            # Fallback: find less specific matches\n",
    "            partial_matches = self.data[\n",
    "                (self.data['Ad Type'] == ad_type_idx) & \n",
    "                (self.data['Ad Topic'] == ad_topic_idx) & \n",
    "                (self.data['Ad Placement'] == ad_placement_idx)\n",
    "            ]\n",
    "            \n",
    "            if len(partial_matches) > 0:\n",
    "                reward = partial_matches['Conversion Rate'].mean()\n",
    "            else:\n",
    "                # Very unlikely, but just in case\n",
    "                reward = self.data['Conversion Rate'].mean() * 0.5  # Penalty for unknown\n",
    "        \n",
    "        # Move to a new random user profile\n",
    "        self.current_idx = np.random.randint(0, self.num_users)\n",
    "        next_state = self.data.iloc[self.current_idx][state_columns].values\n",
    "        \n",
    "        # Assume episodes don't end\n",
    "        done = False\n",
    "        \n",
    "        return next_state, reward, done\n",
    "    \n",
    "    def get_action_space_size(self):\n",
    "        return (self.num_ad_types, self.num_ad_topics, self.num_ad_placements)\n",
    "\n",
    "# Build multi-headed DQN model\n",
    "def build_multi_head_model(state_size, num_ad_types, num_ad_topics, num_ad_placements):\n",
    "    input_layer = Input(shape=(state_size,))\n",
    "    \n",
    "    # Shared layers\n",
    "    x = Dense(128, activation='relu')(input_layer)\n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    \n",
    "    # Separate heads for each action component\n",
    "    ad_type_head = Dense(64, activation='relu')(x)\n",
    "    ad_type_output = Dense(num_ad_types, activation='linear', name='ad_type')(ad_type_head)\n",
    "    \n",
    "    ad_topic_head = Dense(64, activation='relu')(x)\n",
    "    ad_topic_output = Dense(num_ad_topics, activation='linear', name='ad_topic')(ad_topic_head)\n",
    "    \n",
    "    ad_placement_head = Dense(64, activation='relu')(x)\n",
    "    ad_placement_output = Dense(num_ad_placements, activation='linear', name='ad_placement')(ad_placement_head)\n",
    "    \n",
    "    model = tf.keras.Model(inputs=input_layer, outputs=[ad_type_output, ad_topic_output, ad_placement_output])\n",
    "    \n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=0.0001)\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss={\n",
    "            'ad_type': 'mse',\n",
    "            'ad_topic': 'mse',\n",
    "            'ad_placement': 'mse'\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Initialize environment\n",
    "env = ImprovedMarketingMDP(encoded_data)\n",
    "action_space_size = env.get_action_space_size()\n",
    "state_size = len(state_columns)\n",
    "\n",
    "# Initialize model\n",
    "model = build_multi_head_model(\n",
    "    state_size, \n",
    "    action_space_size[0], \n",
    "    action_space_size[1], \n",
    "    action_space_size[2]\n",
    ")\n",
    "\n",
    "# Define training parameters\n",
    "batch_size = 64\n",
    "gamma = 0.95\n",
    "epsilon = 1.0\n",
    "epsilon_min = 0.01\n",
    "epsilon_decay = 0.995\n",
    "memory = deque(maxlen=10000)\n",
    "\n",
    "# Function to store experiences\n",
    "def remember(state, actions, rewards, next_state, done):\n",
    "    memory.append((state, actions, rewards, next_state, done))\n",
    "\n",
    "# Training loop\n",
    "episodes = 5000\n",
    "rewards_history = []\n",
    "\n",
    "for episode in range(episodes):\n",
    "    state = env.reset()\n",
    "    state = np.reshape(state, [1, state_size])\n",
    "    total_reward = 0\n",
    "    \n",
    "    for time_step in range(100):  # Max time steps per episode\n",
    "        # Epsilon-greedy action selection\n",
    "        if np.random.rand() <= epsilon:\n",
    "            # Random actions\n",
    "            ad_type_action = np.random.randint(0, action_space_size[0])\n",
    "            ad_topic_action = np.random.randint(0, action_space_size[1])\n",
    "            ad_placement_action = np.random.randint(0, action_space_size[2])\n",
    "        else:\n",
    "            # Greedy actions based on Q-values\n",
    "            q_ad_type, q_ad_topic, q_ad_placement = model.predict(state, verbose=0)\n",
    "            ad_type_action = np.argmax(q_ad_type[0])\n",
    "            ad_topic_action = np.argmax(q_ad_topic[0])\n",
    "            ad_placement_action = np.argmax(q_ad_placement[0])\n",
    "        \n",
    "        # Take action and observe result\n",
    "        next_state, reward, done = env.step(ad_type_action, ad_topic_action, ad_placement_action)\n",
    "        next_state = np.reshape(next_state, [1, state_size])\n",
    "        \n",
    "        # Store experience\n",
    "        actions = (ad_type_action, ad_topic_action, ad_placement_action)\n",
    "        rewards = (reward, reward, reward)  # Same reward for all heads\n",
    "        remember(state, actions, rewards, next_state, done)\n",
    "        \n",
    "        # Update state and accumulate reward\n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "        \n",
    "        if done:\n",
    "            break\n",
    "    \n",
    "    # Train from experience replay\n",
    "    if len(memory) >= batch_size:\n",
    "        minibatch = random.sample(memory, batch_size)\n",
    "        \n",
    "        states = np.zeros((batch_size, state_size))\n",
    "        ad_type_targets = np.zeros((batch_size, action_space_size[0]))\n",
    "        ad_topic_targets = np.zeros((batch_size, action_space_size[1]))\n",
    "        ad_placement_targets = np.zeros((batch_size, action_space_size[2]))\n",
    "        \n",
    "        # Prepare batch data\n",
    "        for i, (state_i, actions_i, rewards_i, next_state_i, done_i) in enumerate(minibatch):\n",
    "            # Get current Q-values\n",
    "            current_q_ad_type, current_q_ad_topic, current_q_ad_placement = model.predict(state_i, verbose=0)\n",
    "            \n",
    "            # Get future Q-values for next state\n",
    "            future_q_ad_type, future_q_ad_topic, future_q_ad_placement = model.predict(next_state_i, verbose=0)\n",
    "            \n",
    "            # Unpack actions and rewards\n",
    "            ad_type_action, ad_topic_action, ad_placement_action = actions_i\n",
    "            ad_type_reward, ad_topic_reward, ad_placement_reward = rewards_i\n",
    "            \n",
    "            # Update targets using Bellman equation\n",
    "            ad_type_targets[i] = current_q_ad_type[0]\n",
    "            ad_topic_targets[i] = current_q_ad_topic[0]\n",
    "            ad_placement_targets[i] = current_q_ad_placement[0]\n",
    "            \n",
    "            if not done_i:\n",
    "                # Apply reward plus discounted future max Q-value\n",
    "                ad_type_targets[i, ad_type_action] = ad_type_reward + gamma * np.max(future_q_ad_type)\n",
    "                ad_topic_targets[i, ad_topic_action] = ad_topic_reward + gamma * np.max(future_q_ad_topic)\n",
    "                ad_placement_targets[i, ad_placement_action] = ad_placement_reward + gamma * np.max(future_q_ad_placement)\n",
    "            else:\n",
    "                # If done, just use the reward\n",
    "                ad_type_targets[i, ad_type_action] = ad_type_reward\n",
    "                ad_topic_targets[i, ad_topic_action] = ad_topic_reward\n",
    "                ad_placement_targets[i, ad_placement_action] = ad_placement_reward\n",
    "            \n",
    "            # Store state\n",
    "            states[i] = state_i[0]\n",
    "        \n",
    "        # Train the model on the batch\n",
    "        model.fit(\n",
    "            states, \n",
    "            [ad_type_targets, ad_topic_targets, ad_placement_targets],\n",
    "            batch_size=batch_size, \n",
    "            epochs=1, \n",
    "            verbose=0\n",
    "        )\n",
    "    \n",
    "    # Decay epsilon\n",
    "    if epsilon > epsilon_min:\n",
    "        epsilon *= epsilon_decay\n",
    "    \n",
    "    rewards_history.append(total_reward)\n",
    "    \n",
    "    # Progress reporting\n",
    "    if episode % 1 == 0:\n",
    "        avg_reward = np.mean(rewards_history[-100:])\n",
    "        print(f\"Episode: {episode+1}/{episodes}, Avg Reward: {avg_reward:.4f}, Epsilon: {epsilon:.4f}\")\n",
    "\n",
    "# Save the model\n",
    "model.save(\"improved_marketing_rl_model.h5\")\n",
    "print(\"Model saved as improved_marketing_rl_model.h5\")\n",
    "\n",
    "# Save metadata for the app\n",
    "metadata = {\n",
    "    \"label_encoders\": {k: {str(key): value for key, value in v.items()} for k, v in label_encoders.items()},\n",
    "    \"inverse_encoders\": {k: {str(key): value for key, value in v.items()} for k, v in inverse_encoders.items()},\n",
    "    \"feature_ranges\": {k: [float(v[0]), float(v[1])] for k, v in feature_ranges.items()},\n",
    "    \"state_columns\": state_columns,\n",
    "    \"action_space_size\": [int(size) for size in action_space_size]\n",
    "}\n",
    "\n",
    "with open(\"model_metadata.json\", \"w\") as f:\n",
    "    json.dump(metadata, f)\n",
    "print(\"Model metadata saved as model_metadata.json\")\n",
    "\n",
    "# Plot training rewards\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(rewards_history)\n",
    "plt.title(\"Training Rewards Over Episodes\")\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Total Reward\")\n",
    "plt.savefig(\"training_rewards.png\")\n",
    "plt.show()\n",
    "\n",
    "# Evaluate model performance\n",
    "def evaluate_model(model, data, num_samples=1000):\n",
    "    results = []\n",
    "    \n",
    "    for _ in range(num_samples):\n",
    "        # Random user profile\n",
    "        user_idx = np.random.randint(0, len(data))\n",
    "        user = data.iloc[user_idx]\n",
    "        \n",
    "        # Get state features\n",
    "        state = user[state_columns].values\n",
    "        state = np.reshape(state, [1, state_size])\n",
    "        \n",
    "        # Get model predictions\n",
    "        q_ad_type, q_ad_topic, q_ad_placement = model.predict(state, verbose=0)\n",
    "        \n",
    "        # Get best actions\n",
    "        best_ad_type = np.argmax(q_ad_type[0])\n",
    "        best_ad_topic = np.argmax(q_ad_topic[0])\n",
    "        best_ad_placement = np.argmax(q_ad_placement[0])\n",
    "        \n",
    "        # Find similar examples in dataset\n",
    "        similar = data[\n",
    "            (data['Ad Type'] == best_ad_type) & \n",
    "            (data['Ad Topic'] == best_ad_topic) & \n",
    "            (data['Ad Placement'] == best_ad_placement)\n",
    "        ]\n",
    "        \n",
    "        # Calculate expected reward\n",
    "        if len(similar) > 0:\n",
    "            expected_reward = similar['Conversion Rate'].mean()\n",
    "        else:\n",
    "            expected_reward = data['Conversion Rate'].mean() * 0.5\n",
    "        \n",
    "        # Store result\n",
    "        results.append(expected_reward)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    avg_reward = np.mean(results)\n",
    "    max_reward = np.max(results)\n",
    "    min_reward = np.min(results)\n",
    "    \n",
    "    return {\n",
    "        \"average_reward\": float(avg_reward),\n",
    "        \"max_reward\": float(max_reward),\n",
    "        \"min_reward\": float(min_reward),\n",
    "        \"samples\": num_samples\n",
    "    }\n",
    "\n",
    "# Evaluate the model\n",
    "evaluation = evaluate_model(model, encoded_data)\n",
    "print(f\"Model Evaluation: Average Reward: {evaluation['average_reward']:.4f}\")\n",
    "\n",
    "# Save evaluation results\n",
    "with open(\"evaluation_results.json\", \"w\") as f:\n",
    "    json.dump(evaluation, f)\n",
    "print(\"Evaluation results saved as evaluation_results.json\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
